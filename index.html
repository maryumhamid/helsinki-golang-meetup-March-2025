<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
	<title>GenAI with LangchainGo & Ollama</title>

	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/reveal.min.css">


	<script src="https://cdn.jsdelivr.net/npm/reveal.js/dist/reveal.js"></script>


	<!-- Reveal.js Core Styles -->
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/dist/reveal.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/dist/theme/black.css">

	<!-- Prism.js for Syntax Highlighting -->
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-go.min.js"></script>



	<style>
		/* Keep existing heading text-transform override */
		h1, h2, h3, h4, h5, h6 {
			text-transform: none !important; /* Remove uppercase styling */
		}

		/* Remove extra lines from <pre> and <code> */
		pre, code {
			font-size: 0.6em !important; /* Adjust for better fit */
			line-height: 1.2 !important; /* Improve readability */
			padding: 10px !important; /* Ensure proper spacing inside the box */
		}

		/* Enable auto-resize for sections */
		.reveal section {
			overflow: auto !important; /* Enable scrolling for overflowing content */
			padding: 0; /* Remove extra padding */
		}

		/* Prevent unnecessary space in syntax highlighting */
		code[class*="language-"] {
			white-space: pre !important; /* Maintain preformatted text */
		}

		ul ul {
			font-size: 80%;
		}

		.reveal {
			background-color: #c59d6f !important; /* Ensures full background */
		}
		.reveal .slides section {
			background-color: transparent !important; /* Remove per-slide background */
		}
		.reveal pre {
			/* Increase box width and height */
			width: 90% !important; /* Adjust as needed */
			height: auto !important; /* Expand height dynamically */
			max-width: 100% !important; /* Prevent overflow */
			max-height: 600px !important; /* Adjust max height */
			display: block;
		}
		.reveal .slides section {
			background-color: transparent !important;
		}
		@media print {
			* {
				-webkit-print-color-adjust: exact !important;
				print-color-adjust: exact !important;
			}

			/* ‚úÖ Remove any Reveal.js default margins */
			html, body {
				margin: 0 !important;
				padding: 0 !important;
				width: 100% !important;
				height: 100% !important;
				background-color: #c59d6f !important; /* Your background color */
			}

			/* ‚úÖ Make sure Reveal.js fills the entire page */
			.reveal {
				width: 100vw !important;
				height: 100vh !important;
				background-color: #c59d6f !important;
				margin: 0 !important;
				padding: 0 !important;
				box-shadow: none !important;
				border: none !important;
			}

			/* ‚úÖ Ensure slides take full page width and height */
			.reveal .slides {
				width: 100vw !important;
				height: 100vh !important;
				background-color: #c59d6f !important;
				margin: 0 !important;
				padding: 0 !important;
			}

			/* ‚úÖ Fix individual slides */
			.reveal .slides section {
				width: 100vw !important;
				height: 100vh !important;
				background-color: #c59d6f !important;
				color: white !important;
				box-shadow: none !important;
				border: none !important;
				margin: 0 !important;
				padding: 0 !important;
			}

			/* ‚úÖ Fix black gaps between slides */
			.pdf-page {
				margin: 0 !important;
				padding: 0 !important;
				page-break-before: always !important;
				background-color: #c59d6f !important;
			}

			/* ‚úÖ Force background on code blocks too */
			pre, code {
				background-color: #1e1e1e !important; /* Your dark theme for code */
				color: white !important;
			}
		}
	</style>


</head>
<body>
<div class="reveal">
	<div class="slides">
		<!-- Title Slide -->
		<section>
			<h2>GenAI with LangchainGo & Ollama üöÄ</h2>
			<h3>Running AI Models Locally in Go</h3>
			<p><small>Golang Meetup | Helsinki</small></p>
			<p><small>Maryum Hamid</small></p>


		</section>

		<section>
			<h2>Introduction</h2>
			<p><small>Software Engineer at Zalando</small></p>
			<p><small>Preowned premise of Zalando</small></p>
			<p><small>C++ -> Python -> Go</small></p>
		</section>

		<!-- What is Ollama? -->
		<section>
			<h2>Ollama</h2>
			<p>Docker for LLMs</p>
			<p>Fully local inference</p>
			<p>Works offline (perfect for privacy-focused applications)</p>
			<p>Written in Go, uses llama.cpp</p>
			<img src="images/ollama.png" alt="Ollama Logo" width="150"
				 style="position: absolute; top: 10px; right: 10px; border-radius: 10px;">
		</section>

		<!-- Ollama Demo -->
		<section>
			<h2>Ollama demo</h2>
			<pre><code class="language-bash">
        > ollama pull mistral

        > ollama run mistral

        >>> What is Ollama?

        >>> /bye
            </code></pre>
		</section>

		<!-- Supported Models -->
		<section>
			<h2>Supported Models</h2>
			<p>Llama by Meta</p>
			<p>Mistral</p>
			<p>Gemma by Google</p>
			<p>Phi by Microsoft</p>
			<p>Qwen by Alibaba</p>
			<p>DeepSeek</p>
		</section>

		<!-- What is LangchainGo? -->
		<section>
			<h2>LangChainGo</h2>
			<p>Framework for working with LLMs</p>
			<p>Inspired by LangChain (Python)</p>
			<p>Abstraction over AI models</p>
			<p>Works with multiple LLMs</p>
			<pre><code class="language-bash">
        	go get github.com/tmc/langchaingo
            </code></pre>
		</section>

		<!-- Basic Example -->
		<section>
			<h3>Ollama + LangchainGo</h3>
			<pre><code class="language-go">
package main
import (
	"context"
	"fmt"
	"github.com/tmc/langchaingo/llms/ollama"
	"log"
)

func main() {
	llm, err := ollama.New(ollama.WithModel("mistral:latest"))
	if err != nil {
		log.Fatal(err)
	}

	resp, err := llm.Call(context.Background(), "Tell me a joke about Golang.")
	if err != nil {
		log.Fatal(err)
	}

	fmt.Println(resp)
}
		</code></pre>
		</section>

		<section>
			<h2>Memory (Remembers Past Messages)</h2>
			<p>LLMs to remember past interactions</p>
			<p>Maintains Conversation Context </p>
			<p>Buffer Memory, Conversation Memory, Vector-based Memory </p>

		</section>

		<section>
			<h3>Chatbot Demo With Memory</h3>
			<pre style="font-size: 0.4em;"><code class="language-go">
		err = chatMemory.AddUserMessage(context.Background(), input)
		if err != nil {
			continue
		}

		messages, err := chatMemory.Messages(context.Background())
		if err != nil {
			continue
		}

		var conversationHistory string
		for _, msg := range messages {
			conversationHistory += fmt.Sprintf("%s: %s\n", msg.GetType(), msg.GetContent())
		}

		response, err := model.Call(context.Background(), conversationHistory, llms.WithTemperature(0.7))
		if err != nil {
			continue
		}

		err = chatMemory.AddAIMessage(context.Background(), response)
		if err != nil {
			continue
		</code></pre>
		</section>

		<!--		<section>-->
		<!--			<h2>Retrieval-Augmented Generation (RAG)</h2>-->
		<!--			<p>Enhances AI by combining ‚Äúretrieval‚Äù + ‚Äúgeneration‚Äù.</p>-->
		<!--			<p>Responses by retrieving relevant external data</p>-->
		<!--			<p>Works with databases, APIs, and vector stores to fetch the most relevant data.</p>-->
		<!--			<p></p>-->

		<!--		</section>-->

		<section>
			<h2>Image Recognition</h2>
			<pre><code class="language-go">
			llm, err := ollama.New(ollama.WithModel("llava"))
				if err != nil {
					log.Fatal("Error initializing OpenAI:", err)
				}

			messages := []llms.MessageContent{
				{
					Role: llms.ChatMessageType("human"),
					Parts: []llms.ContentPart{
						llms.TextPart("Recognize the object in the image"),
						llms.BinaryPart("dog.jpeg", imageData),
					},
				},
			}

			out, err := llm.GenerateContent(ctx, messages)
			if err != nil {
				log.Fatal("Error generating response:", err)
			}
			</code></pre>

			<p>LLaVA (Large Language and Vision Assistant) is a multimodal model.</p>

		</section>

		<section>
			<h2>Image Recognition Demo</h2>
			<img src="images/dog.jpg" alt="">
			<pre><code class="language-text">
AI Response:  The image shows a golden retriever dog standing in a field of yellow flowers,
likely dandelions. The dog is looking towards the camera with its tongue out, and it appears
to be enjoying the sunny day. The background suggests it might be late afternoon or early evening,
given the warm lighting and the shadows cast by the dog and the flowers.
			</code></pre>
		</section>

		<section>
			<h2>Thank you!</h2>
			<p><a href="https://www.linkedin.com/in/maryum-hamid-6657231aa/" style="font-size: 0.5em; color: white;">https://www.linkedin.com/in/maryum-hamid-6657231aa/</a></p>
		</section>
	</div>
</div>

<script>
	document.addEventListener("DOMContentLoaded", function() {
		Reveal.initialize({
			controls: true,
			progress: true,
			center: true,
			slideNumber: true,
			pdfSeparateFragments: false,  // Ensures fragments are printed on the same page
			pdfMaxPagesPerSlide: 1,       // One slide per PDF page
			width: 1900,                  // Ensures full-width slides
			height: 1080,                 // Ensures full-height slides
			viewDistance: 50,             // Ensures all slides are pre-rendered before printing
			transition: 'none',           // Prevents animations from affecting print layout
			backgroundTransition: 'none'
		});
	});
</script>

</body>
</html>
